% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\makeatletter
\let\@copyrightspace\relax
\makeatother

\usepackage{tabularx}
\usepackage{url}

\begin{document}

\title{Accountability and Privacy in the Modern Internet}
\subtitle{Comsys Seminar Paper WS 2015/16}

\numberofauthors{1}
\author{
\alignauthor
Lucas Braun\\
	RWTH Aachen University \\
       \email{lucas.braun@rwth-aachen.de}
}

\maketitle
\begin{abstract}
%Your typical abstract.
\end{abstract}

\section{Introduction}
The history of the modern Internet as we know it today goes back as far as the early eighties -- IPv4, ICMP and TCP, which where all specified in 1981, together with 1984's DNS, still are the foundation on which all Internet traffic is being transported.

The way these technologies are designed is heavily reflecting the challenges that were of importance at that time -- namely "the creation of a distributed communication network that is robust against packet loss and other network failures; support across multiple types of networks and communication services; and the management of Internet resources in a cost-effective and distributed way" \cite{mot}.

As time went by and the Internet began to grow bigger and bigger, new challenges that where not incorporated into these original design goals arose. The following work will focus on two of the biggest of those: accountability and privacy.

While there is already a plethora of technologies in use that enable for varying levels of privacy, e.g., Tor and NAT, accountability is on a completely different page. Not only is accountability hard to guarantee, but it tends to be mutually exclusive with privacy. This is largely due to the fact that the first usually means strengthening sources addresses while the latter usually means weakening them \cite{apip}.

To delve deeper into these topics, section~\ref{sec:acc} explains what accountability is, why it is needed and by whom and present some recent research projects while section~\ref{sec:priv} does the same for privacy. Furthermore, section~\ref{sec:apip} presents the \emph{Accountable and Private Internet Protocol}, a recent attempt at creating a solution that balances between accountabilty and privacy \cite{apip}.

[...]

%Explain what accountability and privacy actually are in the context of internet, motivate why anyone would need it (maybe case examples), who would need it and why no one has it.

\section{Accountability}
\cite{bootstrapping} \cite{mirkovic}
\label{sec:acc}
A lot of crime and chaos in the real world is being prevented because we have mechanisms in place that associate distinct persons or group of persons with actions, making everyone accountable for most of what they are doing. The Internet, as we have it today, is largely devoid of such mechanisms. Techniques like source IP spoofing can give everyone easy anonymity and enables for malicious actions such as denial of service attacks without having to fear any consequences.

Naylor et al. \cite{apip} define three key properties that must be fulfilled in an accountable internet:
\begin{itemize}
\item For every packet there exists an entity that takes responsibility if the packet is of malicious nature
\item If a malicious flow is detected, it can be stopped quickly
\item Subsequent mischief can be prevented by punishment or expulsion
\end{itemize}

Accountability would thus be a welcomed trait in the Internet, as it can greatly help to stop and even prevent attacks. What is needed to provide such accountability is some efficient way to verify the alleged identity of a source, something that is not done directly on the network layer of today's internet. 

Approaches to fix these shortcomings exist, but many of them come with their own shortcomings, rendering them not ideal for real world deployment, e.g., complicated mechanisms changing "the free-access model of the Internet", the need for external sources of trust, or the need for manual filtering by network operators \cite{aip}.

%There are a lot of cool papers. AIP must be explained, maybe IPA or Mirkovic and Reiher as a second approach (or even both?).
\subsection{Accountable Internet Protocol}
\label{sec:aip}
Andersen et al. propose AIP \cite{aip}, the \emph{Accountable Internet Protocol}, as a replacement for the current IP. Hosts and domains using AIP are enabled to prove their identity without the need for an external trusted authority.

\subsubsection{Basic design}
\label{sec:aipbd}

In AIP, the transition to prefixed CIDR-style addresses is reverted in favor of a simpler multi component address. Each independently administered network gets divided into one or more \emph{accountability domains} (ADs) by its administrative unit, providing each AD with a globally unique identifier. Likewise, every end host must be provided a globally unique EID identifier, leading to an address scheme of \texttt{AD:EID}. In case of need for hierarchically organized accountability domains, the scheme can be extended to \texttt{AD$_1$:AD$_2$:\ldots:AD$_k$:EID}. Furthermore, if a host connects to an accountability domain via multiple network interfaces, each interface is provided its own EID by altering the \emph{interface bits} -- the last eight bits of the EID.

\begin{figure}[h!]
	\label{fig:aipadr}
	\newcolumntype{C}{>{\centering}X}
	\begin{tabularx}{0.47\textwidth}{|c|C|c|}
		\hline & & \\
 		Crypto vers & Public key hash & Interface \\
		8 bits & 144 bits & 8 bits \\ & & \\
		\hline
	\end{tabularx}
	\caption{AIP addresses consist of a version number, a hash of a public key and an interface identifier 		(which is set to zero for accountability domains) \cite{aip}}
\end{figure}

This simplification of the addressing enables for \emph{self-certifying} addresses: the main part of the 160 bit long AIP address is simply a 144 bit long hash of the endpoint's respective accountability domain's public key (Figure \ref{fig:aipadr}).

\subsubsection{Verification}

Now, when an end point tries to send a packet, it is the first-hop router's duty to verify the end point's identity. When a packet from a not-verified host arrives, the packet is dropped and the router replies with a \emph{verification packet} V. This packet is signed with a periodically created router specific secret. The sending host signs this packet as well, returns it to the router, which can now verify the sender's identity with the help of his private key, permitting the sender to resend the dropped packet as well as subsequent packets.


\section{Privacy}
\label{sec:priv}
Privacy in the Internet that everyone of us uses day to day has increasingly come into the spotlight during the past years. Even more so since the 2013 global surveillance disclosure initiated by whistle blower Edward Snowden, which has unprecedentedly shown how bad the situation really is.

The "I've Got Nothing To Hide" argument often heard in this context is a fallacy that stems from the misunderstanding that privacy is the hiding of a wrong \cite{solove}. Imagining an Internet without privacy, it would for example be conceivable that health insurances higher their premium for insurance holders who look up certain diseases or, say, order a book on cancer in one week and a wig in the next one.

Mechanisms to protect a users privacy on a network level are manifold and so are their modes of operation. Most of them focus on the two main characteristics of privacy: anonymity and security. Anonymity is granted when an entity is unidentifiable within other entities, building an \emph{anonymity set}. Security, on the other hand, requires the data sent over a network to be masked from eavesdroppers, which is usually done via encryption. Solution that provide either one or both of these characteristics exist; Some more commonly used are the following:
\begin{itemize}
\item \textbf{Proxy servers} let you route traffic over them, which will hide your IP address from simple checks but do not provide any privacy beyond.
\item \textbf{Virtual Private Networks} (VPNs) provide a fully encrypted tunnel to the internet and thus a maximum of anonymity and security. They can however be slow and users' real privacy is depends on the discretion of the VPN's provider.
\item \textbf{Onion Routing} redirects traffic over a random route of proxies, accessed over so called rendezvous points. The data is encrypted several times by the client and each proxy \emph{peals off} one encryption, like the layers of an onion. Analogously, responses are getting encrypted once by each proxy and can than be decrypted by the client. Online Routing, while a little bit slow, is considered relatively secure. Research has unveiled quiet a few possible attacks and weaknesses, many of which are based on controlling the first and the last proxy in an onion route. A popular example implementation of onion routing is Tor.
\end{itemize}
\subsection{Tor instead of IP}
Liu et al. propose using Tor not on top of IP, but instead of IP \cite{tor}. In their approach, ISPs still build the foundation of the network, each managing its own domain and relationships to each other. What differs from the traditional Internet is that the \emph{Border Gateway Protocol} (BGP) gets replaced by onion routers and rendezvous mailboxes, what leads to all communication on the network being over onion routes, without exception.

ISPs advertise pathlets \cite{pathlet} of the form \texttt{<ingress ISP, transit ISP, egress ISP>} for pairs of neighboring ISPs. End hosts then define a path for their packets using these triplets and a mailbox address. The actual onion routing is done by zigzagging in the so called core network, which consists only of the biggest ISPs, which can handle the bandwidth overhead better than smaller ISPs. Packets that do not need anonymity simply do not zigzag in the core.

While this approach provides a high amount of anonymity to every host on the Internet, this is probably also its biggest drawback. It disposes the Internet of any accountability mechanism and makes censorship hard, two traits that are unlikely to persuade ISPs and governments to deviate from IP, thus leaving Tor instead of IP to be a neat thought experiment.
\subsection{Lightweight Anonymity and Privacy}
To close the gap between system that provide slow but strong anonymity, like Tor or mix networks, and not using any privacy enhancement at all, Hsiao propose \emph{Lightweight Anonymity and Privacy} (LAP) \cite{lap}. The authors aim for masking the identity against servers (rather than globally, as Tor, etc. do) while introducing as little latency as possible in return.
\subsection{BLIND}
\cite{blind}  

\section{Accountable and Private Internet Protocol}
\label{sec:apip}

With the \emph{Accountable and Private Internet Protocol} (APIP) \cite{apip}, as the name suggests, Naylor et al. try to find a way to balance between the seemingly mutually exclusive properties accountability and privacy. To achieve this goal they borrow heavily from AIP (see section \ref{sec:aip}), but delegate the job of verifying identities to so called \emph{accountability delegates}. The way this is done opens up the possibility to mask ones return address and thus allows for the application of privacy preserving methods like end-to-end encryption.

Accountability, as we have seen, usually comes with a strengthened source address while privacy implicates weakened or masked source addresses. The basic idea that enables APIP to cater the needs of both functionalities is to separate the source address into an accountability address and a return address. It is now no longer necessary to know who sent a packet in order to be able to verify that it is accounted for, since verification is done exclusively with the help of the accountability delegate.

\subsection{Addressing and packet flow}

In APIP, addresses are always of the form \texttt{NID:HID:SID}. The three components of such an address are as follows:
\begin{itemize}
\item \emph{network id} \texttt{NID}: used to identify the correct domain
\item \emph{host id} \texttt{HID}: used to find the host inside of it's domain. The \texttt{HID} is assumed to be self-certifying, as explained in section \ref{sec:aipbd}
\item \emph{socket id} \texttt{SID}: to further specify the host's socket used for the packet
\end{itemize}
Notice that this doesn't make any assumptions about the underlying protocol, but rather is a generic addressing scheme which is applicable to most protocols. For example, when applied to IP, \texttt{NID} and \texttt{HID} are represented by an IP address while \texttt{SID} corresponds to the port number. However, the assumption about the \texttt{HID} being self-certifying must be relaxed to accommodate APIP with IP, which in turn leads to the need for a Public-Key-Infrastructure. 

Depending on it's type, each packet uses either two or three APIP addresses. The two mandatory ones are the \emph{destination address} and the \emph{accountability address}, which is pointing to the accountability delegate. The optional third address is the \emph{return address}, which is used if a response to the packet is expected and is not needed otherwise.

An implication of this approach is that flow IDs might not be fine granular enough if a single accountability delegate vouches for several clients, but this can be fixed by either using distinct SIDs for every client and/or by adding a flow ID field to the packet header. How this is handled does have an impact on both privacy and flow control; while just allocating the same flow ID to every client might provide maximum anonymity, it can also lead to a lot of non-malicious traffic being blocked when only a single client misbehaves. Then again, assigning a unique flow ID to every client might provide very fine flow control but it also makes it easy to map traffic to specific senders. A good trade off between these extremes is posed by assigning each client not a single unique flow ID but a pool of enough flow IDs to ensure a sufficient level of anonymity. 

\subsection{Accountability mechanism}

The verification process is somewhat similar to AIP, but additionally uses a level of indirection: When sending a package, the sender also informs it's accountability delegate about said package by calling the delegate's \texttt{brief(packet, clientID)} interface. Any on-path router as well as the packet's receiver can act as a verifier by calling the delegate's \texttt{verify(packet)} interface. Verifiers are than free to filter traffic that can not be verified and furthermore receivers can use the delegate's \texttt{shutoff(packet)} interface to permanently keep the delegate from vouching for a specific flow.

\subsubsection{Brief()}
Informing the accountability delegate about new packets is done by either briefing each packet individually with the transmission of the packet's fingerprint or by periodically briefing them in bulk with the transmission of a bloom filter of several fingerprints. The fingerprints themselves are of the following form:
\begin{equation}
F(P) = H(K_{SD_S} || P_{header} || H(P_{body} ))
\end{equation}
where $H$ is a cryptographic hash function and $K_{SD_S}$ is a symmetric key only known to the sender and the delegate.

To circumvent the need for verification on the briefs, a token field is included in the briefs' header. This could for example be a hash chain of a shared secret between sender and delegate. The delegate will know that is has to look for this token, since briefs' accountability and destination addresses are identical.

\paragraph{Recursive Verification}
Instead of using the aforementioned briefing mechanism, it is also possible to do recursive verification and merely use the accountability delegate as a middleman. In this case, \texttt{brief()} is not used. Instead, the delegate forwards all calls to \texttt{verify()} to the packet's alleged sender who in turn has to decide whether he indeed sent the packet or not and respond accordingly to the delegate. While this approach saves network and storage overhead it also has a drawback; in order for this to work, every flow ID must be mapped to exactly one client, potentially reducing client anonymity.

\subsubsection{Verify()}
When a packet passes a router, it checks whether the packet's flow ID has already been verified during the current \emph{verification interval}. At the end of each interval all verifications get purged. If it has not yet been verified, the router will verify the packet with the accountability delegate specified in the header and may choose to drop the packet and inform the sender about the ongoing verification process. The accountability delegate than checks whether the packet in question has been briefed by it's sender, the used \texttt{SID} is associated with the sender the connection between sender and receiver has not been blacklisted with a call to \texttt{shutoff()}. In case that all three conditions are met, the verifier adds the flow to it's whitelist for the current verification interval.

\subsubsection{Shutoff()}
As previously mentioned, receivers can use a call to \texttt{shutoff()} to tell an accountability delegate that they do not wish to further receive transmissions from one of their clients. In addition to blacklisting and thus preventing the client from sending to this particular receiver, the delegate may contact its client about the malicious traffic that is coming from him to work on a solution. Depending on whether the client is a victim himself (i.e., is compromised by a bot net) or was sending malicious traffic on purpose, the delegate might even withdraw from their contract and/or report him if he is breaking law.

\subsection{Privacy mechanism}
In APIP, privacy is achieved by masking the return address used in the protocol's header information. Since the return address is not needed when forwarding packets, nor for APIP's accountability mechanism, various masking techniques can be applied, depending on who it shall be hidden from. While APIP does not dictate any specific techniques, the authors give two examples:
\begin{enumerate}
\item \textbf{End-to-end Encryption} hides the return address from everyone but the receiver. This could either be done directly on the network layer or be moved higher up like it is done in IPsec. 
\item \textbf{Network Address Translation} can be used to hide the sender's address not only from the network, but also from the receiver. This is not a problem since this does not affect accountability. However, it must be considered that this will narrow down the anonymity set, especially so if NAT is only done at the first hop.
\end{enumerate}
An obvious way to increase privacy in APIP is to drop the return address altogether for packets that do not necessitate them or only send it with the first packet of a flow and rely on the receiver to save it for later reuse.

\subsection{Weaknesses}
The shortcomings of APIP mainly stem from the dependency for trustworthy accountability delegates. A trustworthy delegate will perform three main duties free of failure:
\begin{itemize}
\item Protect its clients' privacy
\item Verify packets that has been briefed for, and only these
\item Reacting correctly to \texttt{shutdown()} calls 
\end{itemize}
Not only can delegates gather (and possibly release) a lot of data about each of its clients communication partners (albeit they only see the packet headers and never the packet contents), but it can also go rogue by deliberately not verifying any packets from a chosen client, effectively blocking communication for him, or by verifying packets that where not briefed for and ignoring calls to \texttt{shutdown()}.

To combat misbehaving delegates, the authors propose to have a central authority which monitors a list of trusted delegates. Verifiers should only accept packets with a whitelisted accountability address and rogues are simply removed. If the central authority is realized in a way that ensures that everyone can trust them this is an acceptable solution. Otherwise this approach is just chasing its own tail.

The alternative to allow every host to behave as an accountability delegate for itself (like in AIP) or anyone else seems to be worse, since it leaves doors wide open for botnet attacks, whose prevention are one of the main motivations of APIP. However, the authors admit that there needs to be further discussion on this topic and a feasible solution probably lies somewhere in between.

\subsubsection{Flooding Attacks}
APIP is prone to two different kinds of flooding attacks:
\begin{itemize}
\item \emph{Verification-Flooding} There is not really anything preventing hosts from flooding delegates with verification requests. Only the delegate can decide which requests are forged and which are honest. Delegates will need an efficient way to inform the attacker's source domain about misbehaving hosts.
\item \emph{Brief-Flooding} In an analogous manner, clients can occupy their delegate by spamming them with briefs. To differentiate attacks from very active clients, delegates could enforce rules like only sending bloom filter periodically if their briefs reach a certain threshold.
\end{itemize}

\subsection{Evaluation}
\begin{table*}[t]
	\label{tab:evalcomparison}
	\newcolumntype{L}{>{\left}X}
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{lXXX}
		\hline 
 		& \textbf{Computational Overhead} & \textbf{Storage Overhead} & \textbf{Bandwidth Overhead} \\ \hline
		\texttt{brief()} & 2-3 hashes p.p. (per packet) & 20 bytes p.p. & 2.5\% \\ 
		\texttt{brief()} bloom filters & 2-3 hashes p.p. & one bloom filter per client & 0.25\%-0.5\% \\ 
		\texttt{verify()} & $\le$78.000 lookups in table with 1.5 million entries at a 10 second purge interval & 120 bytes per flow, $\le$94MB at a 10 second purge interval & \\ 
		\hline
	\end{tabularx}
	\caption{Computational, storage and bandwidth overhead for APIP's main functions \texttt{brief()} with and without using bloom filters, and \texttt{verify()}.}
\end{table*}
When considering the use of APIP in a real world scenario, we need to know how well it performs. How much overhead does it introduce in regards of computational power, storage and bandwidth? To answer these questions, the authors took a 5 minute trace with NetFlow from the border routers of their university network, gathering ten million flows, and extrapolated APIP's overhead from this data set. Unfortunately, they only call the network 'mid-sized', without stating a specific network size.

\subsubsection{Brief()}
The overhead of \texttt{brief()} is heavily dependent on whether raw fingerprints are sent or they are being aggregated as bloom filters before being sent. Regardless, clients will have to compute two hashes needed for every fingerprint. Additionally one more hash is needed for the computation of the MAC used in the \texttt{brief()}, totaling to 2-3 hashes depending on whether bloom filters are used and if they are, how many packets get briefed with a single filter. While this very reasonable for end hosts, it might not be so reasonable anymore for big servers, which could be slowed down or even topped out by the amount of hashes that must be computed for high packet numbers. Even high performing CPUs usually do not reach 100 MH/s \cite{hashes}, but ASICs used for mining cryptocurrencies are powerful enough to close this gap.

Since briefs are only saved at the accountability delegates, overhead will only happen there. The amount of space that is needed depends on the chosen expiration interval as well as whether bloom filters are used or not. For saving the plain fingerprints 20 bytes are needed per packet, which is the size of the corresponding SHA-1 hash. With bloom filters the demand for storage can be narrowed down to a single filter per client.

Analogously, bandwidth overhead stays much lower when bloom filters are used; an overhead of 2.5\% for sending fingerprints can be reduced to 0.25\%-0.5\% with the use of filters, assuming that a new filter is sent out every second. However, the recorded trace did not include local addresses, making filtering on a per flow basis rather than per client necessary. A real world number could thus be even lower for the variant using filters.

\subsubsection{Verify()}
Verification overhead is happening in the form of computational overhead at the delegates and as storage overhead at the verifying routers. Both is dependent on the chosen verification interval: while storage overhead (and also the time until a \texttt{shutoff()} takes effect) grows with interval length, delegates get to respond to less calls to \texttt{verify()}. The gathered data suggests that an interval length of ten seconds is a good trade off, since most flows aren't longer than this (making re-verification of most flows unnecessary) and it is still short enough to keep storage requirements at verifiers reasonable.

At a ten second interval length, given the recorded trace, delegates would process $\le$78.000 \texttt{verify()}s a second, leading to that many lookups in a table with 1.5 million entries, checking for the corresponding \texttt{brief()}, and a single signature generation for signing the response each. Considering that those numbers assume that the whole university network incorporates only a single accountability delegate this sounds acceptable, but without knowledge of the exact network size it is hard to discuss this.

The storage overhead at verifiers stems from the need to maintain whitelists about flows that have already been verified. Each entry in a whitelist accommodates two host addresses of the known form \texttt{NID:HID:SID}. To comply with the assumption that addresses are self-certifying, the addresses are assumed to be three times the size of a SHA-1 hash (which is 20 bytes), leading to a size of 120 bytes per whitelist entry. For the recorded data and a ten second interval for purging the list, storage requirements are $\le$94 MB for every verifier.

\subsubsection{Shutoff()}
Since a \texttt{shutoff()} only takes effect for a specific flow after at least one whitelist on the route has been purged, the delay scales linearly with interval length and can be reduced by forcing more routers to verify traffic.

\section{Summary}

\section{Conclusion}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{apip}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\balancecolumns
% That's all folks!
\end{document}
